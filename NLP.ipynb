{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNc8uwJtOcBsROtiuXzoklo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UgzfDn2lgiF",
        "colab_type": "text"
      },
      "source": [
        "# [Natural Language Processing Overview](https://www.youtube.com/watch?v=5ctbvkAMQO4)\n",
        "### NLTK - Main tools for NLP\n",
        "TEXT MINING/ANALYTICS = process of deriving meaningful information from natural language text \n",
        "*   involves structuring input text\n",
        "*   deriving patterns\n",
        "*   evaluating & interpreting the output\n",
        "\n",
        "NLP = part of cs & AI which deals with human languages\n",
        "\n",
        "Applications of NLP\n",
        "*   Sentiment analysis \n",
        "*   Chatbot\n",
        "*   Speech recognition\n",
        "*   Machine translation\n",
        "*   spell checking, keyword search, info extraction, advertisement matching (recommendation of ads based on history)\n",
        "\n",
        "Components of NLP\n",
        "- **natural language understanding** = analyze and find useful info from input\n",
        "  - mapping input to useful representations\n",
        "  - analyzing diff aspects of the language\n",
        "- **natural language generation** = producing meaningful phrases \n",
        "  - text planning\n",
        "  - sentence planning\n",
        "  - text realiation\n",
        "\n",
        "Processes of NLP\n",
        "- **Tokenization** = breaking sentences into smaller useful units (i.e. words) \n",
        "  1. Break a complex sentence into words\n",
        "  2. Understand the importance of each of the words with respect to the sentence\n",
        "  3. Produce a structural description on an input sentence \n",
        "- Stemming = normalize words into its base or root form\n",
        "  - ex. affecation, affects, affections, affecting >>> affect \n",
        "  - stemming algorithms usually take into account taking off the beginning or end, whilst taking into account a list of common prefixes, suffixes \n",
        "  - does not always work \n",
        "- Lemmatization = morphological analysis of the word\n",
        "  - need to have a detailed dictionary that the algorithm can look through to link the form back to its original word/**lemma** \n",
        "  - **lemma** = root word \n",
        "  1. groups together different inflected forms of a word, called Lemma \n",
        "  2. somehow similar to **Stemming**, as it maps several words into one common root\n",
        "  3. Output of lemmatisation is a proper word \n",
        "    - ex. using Lemmatization: mapping gone, going, went >> **go**\n",
        "    - this would not be the same output when using stemming \n",
        "- POS tags\n",
        "  - once we have tokens & divided them into its root forms, then POS tags\n",
        "  - **POS Tags** = parts of speech, ex. nouns, pronounds, verbs etc. \n",
        "  - how the word functions as **meaning** and **grammatically** within the sentence \n",
        "  - **Issue** : * \"Google\" it on the web *\n",
        "    - Google is a proper noun but its used as a verb here \n",
        "- Named Entity Recognition (NER) \n",
        "  - helps to overcome issues involved with POS \n",
        "  - takes name entity (ex. person name, company names, location, quantity etc.) & has 3 steps \n",
        "  1. noun phrase identification \n",
        "  2. phrase classification\n",
        "  3. entity disambiguation \n",
        "  - ex. Google's CEO Sundar introduced the new phone at New York Central Mall\n",
        "    - Organization: Google, Central Mall\n",
        "    - Person: Sundar\n",
        "    - Location: New York\n",
        "- Chunking = picking up individual pieces of info & grouping them into bigger pieces (i.e. **chunks**)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ1yIMMAy_ZW",
        "colab_type": "text"
      },
      "source": [
        "# [NLTK Tutorial](https://www.youtube.com/watch?v=05ONoGfmKvA)\n",
        "## Human Language\n",
        "- **Language** = combinations of: alphabets > words > sentences\n",
        "- **Grammar** = rules to create sentences \n",
        "\n",
        "## What is Text Mining?\n",
        "- **Text Mining/ Text Analystics** = process of **deriving** meaningful **information** from natural language **text**\n",
        "- the overall goal, is essentially to turn text into data for analysis, via appliation of NLP\n",
        "\n",
        "## Basic Structure of a NLP Application \n",
        "![](https://www.dropbox.com/s/a0lppcyj2l9126w/Screenshot%202020-06-15%2017.24.59.png?dl=0)\n",
        "- NLP Layer is connected to \n",
        "  1. Knowledge Base - Source Content = ex. chat logs\n",
        "    - shit used to train the algorithms\n",
        "  2. Data Storage - Interaction History & Analytics \n",
        "    - help generate meaningful output \n",
        "\n",
        "## Ambiguity \n",
        "1. Lexical/Semantic = many possible meanings for a single **word**\n",
        "  - Fisherman went to the **bank** : money bank or water bank??\n",
        "2. Syntactic = many possible meanings for a single **sentence**\n",
        "  - The chicken is ready to eat\n",
        "3. Referential = concerning pronounds...what does **he** represent?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0rHme4JlSah",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "f176f058-9b57-40db-a1f9-54b818e63ef0"
      },
      "source": [
        "import os\n",
        "import nltk\n",
        "import nltk.corpus\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "nltk.corpus.gutenberg.fileids()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['austen-emma.txt',\n",
              " 'austen-persuasion.txt',\n",
              " 'austen-sense.txt',\n",
              " 'bible-kjv.txt',\n",
              " 'blake-poems.txt',\n",
              " 'bryant-stories.txt',\n",
              " 'burgess-busterbrown.txt',\n",
              " 'carroll-alice.txt',\n",
              " 'chesterton-ball.txt',\n",
              " 'chesterton-brown.txt',\n",
              " 'chesterton-thursday.txt',\n",
              " 'edgeworth-parents.txt',\n",
              " 'melville-moby_dick.txt',\n",
              " 'milton-paradise.txt',\n",
              " 'shakespeare-caesar.txt',\n",
              " 'shakespeare-hamlet.txt',\n",
              " 'shakespeare-macbeth.txt',\n",
              " 'whitman-leaves.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gfm2c4vF7ImE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "edf77024-4aea-445a-f4a8-22516135d410"
      },
      "source": [
        "hamlet = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
        "hamlet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGI1esms7px_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7b244cbc-1dde-4605-ef10-873d2d4791e5"
      },
      "source": [
        "#first 500 words in hamlet\n",
        "for word in hamlet[:500]:\n",
        "  print(word, sep=' ', end = ' ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ The Tragedie of Hamlet by William Shakespeare 1599 ] Actus Primus . Scoena Prima . Enter Barnardo and Francisco two Centinels . Barnardo . Who ' s there ? Fran . Nay answer me : Stand & vnfold your selfe Bar . Long liue the King Fran . Barnardo ? Bar . He Fran . You come most carefully vpon your houre Bar . ' Tis now strook twelue , get thee to bed Francisco Fran . For this releefe much thankes : ' Tis bitter cold , And I am sicke at heart Barn . Haue you had quiet Guard ? Fran . Not a Mouse stirring Barn . Well , goodnight . If you do meet Horatio and Marcellus , the Riuals of my Watch , bid them make hast . Enter Horatio and Marcellus . Fran . I thinke I heare them . Stand : who ' s there ? Hor . Friends to this ground Mar . And Leige - men to the Dane Fran . Giue you good night Mar . O farwel honest Soldier , who hath relieu ' d you ? Fra . Barnardo ha ' s my place : giue you goodnight . Exit Fran . Mar . Holla Barnardo Bar . Say , what is Horatio there ? Hor . A peece of him Bar . Welcome Horatio , welcome good Marcellus Mar . What , ha ' s this thing appear ' d againe to night Bar . I haue seene nothing Mar . Horatio saies , ' tis but our Fantasie , And will not let beleefe take hold of him Touching this dreaded sight , twice seene of vs , Therefore I haue intreated him along With vs , to watch the minutes of this Night , That if againe this Apparition come , He may approue our eyes , and speake to it Hor . Tush , tush , ' twill not appeare Bar . Sit downe a - while , And let vs once againe assaile your eares , That are so fortified against our Story , What we two Nights haue seene Hor . Well , sit we downe , And let vs heare Barnardo speake of this Barn . Last night of all , When yond same Starre that ' s Westward from the Pole Had made his course t ' illume that part of Heauen Where now it burnes , Marcellus and my selfe , The Bell then beating one Mar . Peace , breake thee of : Enter the Ghost . Looke where it comes againe Barn . In the same figure , like the King that ' s dead Mar . Thou art a Scholler ; speake to it Horatio Barn . Lookes it not like the King ? Marke it Horatio Hora . Most like : It harrowes me with fear & wonder Barn . It would be spoke too Mar . Question it Horatio Hor . What art "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srbs5y-4YLYY",
        "colab_type": "text"
      },
      "source": [
        "#Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8lbZ4vZ8DwM",
        "colab_type": "text"
      },
      "source": [
        "## Demo: Tokenizing A String Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVRkl4be8PDU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "14fe634f-c851-4afa-feac-03125a3edbad"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize \n",
        "rona = \"\"\"The COVID-19 pandemic, also known as the coronavirus pandemic, is an ongoing pandemic of coronavirus disease 2019 (COVID‑19), caused by severe acute respiratory syndrome coronavirus 2 (SARS‑CoV‑2). The outbreak was first identified in Wuhan, China, in December 2019. The World Health Organization declared the outbreak a Public Health Emergency of International Concern on 30 January 2020, and a pandemic on 11 March. As of 15 June 2020, more than 7.96 million cases of COVID-19 have been reported in more than 188 countries and territories, resulting in more than 434,000 deaths; more than 3.8 million people have recovered.\"\"\"\n",
        "\n",
        "rona_tokens = word_tokenize(rona)\n",
        "rona_tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'COVID-19',\n",
              " 'pandemic',\n",
              " ',',\n",
              " 'also',\n",
              " 'known',\n",
              " 'as',\n",
              " 'the',\n",
              " 'coronavirus',\n",
              " 'pandemic',\n",
              " ',',\n",
              " 'is',\n",
              " 'an',\n",
              " 'ongoing',\n",
              " 'pandemic',\n",
              " 'of',\n",
              " 'coronavirus',\n",
              " 'disease',\n",
              " '2019',\n",
              " '(',\n",
              " 'COVID‑19',\n",
              " ')',\n",
              " ',',\n",
              " 'caused',\n",
              " 'by',\n",
              " 'severe',\n",
              " 'acute',\n",
              " 'respiratory',\n",
              " 'syndrome',\n",
              " 'coronavirus',\n",
              " '2',\n",
              " '(',\n",
              " 'SARS‑CoV‑2',\n",
              " ')',\n",
              " '.',\n",
              " 'The',\n",
              " 'outbreak',\n",
              " 'was',\n",
              " 'first',\n",
              " 'identified',\n",
              " 'in',\n",
              " 'Wuhan',\n",
              " ',',\n",
              " 'China',\n",
              " ',',\n",
              " 'in',\n",
              " 'December',\n",
              " '2019',\n",
              " '.',\n",
              " 'The',\n",
              " 'World',\n",
              " 'Health',\n",
              " 'Organization',\n",
              " 'declared',\n",
              " 'the',\n",
              " 'outbreak',\n",
              " 'a',\n",
              " 'Public',\n",
              " 'Health',\n",
              " 'Emergency',\n",
              " 'of',\n",
              " 'International',\n",
              " 'Concern',\n",
              " 'on',\n",
              " '30',\n",
              " 'January',\n",
              " '2020',\n",
              " ',',\n",
              " 'and',\n",
              " 'a',\n",
              " 'pandemic',\n",
              " 'on',\n",
              " '11',\n",
              " 'March',\n",
              " '.',\n",
              " 'As',\n",
              " 'of',\n",
              " '15',\n",
              " 'June',\n",
              " '2020',\n",
              " ',',\n",
              " 'more',\n",
              " 'than',\n",
              " '7.96',\n",
              " 'million',\n",
              " 'cases',\n",
              " 'of',\n",
              " 'COVID-19',\n",
              " 'have',\n",
              " 'been',\n",
              " 'reported',\n",
              " 'in',\n",
              " 'more',\n",
              " 'than',\n",
              " '188',\n",
              " 'countries',\n",
              " 'and',\n",
              " 'territories',\n",
              " ',',\n",
              " 'resulting',\n",
              " 'in',\n",
              " 'more',\n",
              " 'than',\n",
              " '434,000',\n",
              " 'deaths',\n",
              " ';',\n",
              " 'more',\n",
              " 'than',\n",
              " '3.8',\n",
              " 'million',\n",
              " 'people',\n",
              " 'have',\n",
              " 'recovered',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RhOVhGiUuR0",
        "colab_type": "text"
      },
      "source": [
        "## Demo: Find Frequency of Tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiZQNZbQUt4R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c7ec8d89-2846-470a-ad8a-2b13cc59f202"
      },
      "source": [
        "from nltk.probability import FreqDist\n",
        "fdist = FreqDist()\n",
        "\n",
        "for word in rona_tokens:\n",
        "  fdist[word.lower()] += 1\n",
        "fdist"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'(': 2,\n",
              "          ')': 2,\n",
              "          ',': 8,\n",
              "          '.': 4,\n",
              "          '11': 1,\n",
              "          '15': 1,\n",
              "          '188': 1,\n",
              "          '2': 1,\n",
              "          '2019': 2,\n",
              "          '2020': 2,\n",
              "          '3.8': 1,\n",
              "          '30': 1,\n",
              "          '434,000': 1,\n",
              "          '7.96': 1,\n",
              "          ';': 1,\n",
              "          'a': 2,\n",
              "          'acute': 1,\n",
              "          'also': 1,\n",
              "          'an': 1,\n",
              "          'and': 2,\n",
              "          'as': 2,\n",
              "          'been': 1,\n",
              "          'by': 1,\n",
              "          'cases': 1,\n",
              "          'caused': 1,\n",
              "          'china': 1,\n",
              "          'concern': 1,\n",
              "          'coronavirus': 3,\n",
              "          'countries': 1,\n",
              "          'covid-19': 2,\n",
              "          'covid‑19': 1,\n",
              "          'deaths': 1,\n",
              "          'december': 1,\n",
              "          'declared': 1,\n",
              "          'disease': 1,\n",
              "          'emergency': 1,\n",
              "          'first': 1,\n",
              "          'have': 2,\n",
              "          'health': 2,\n",
              "          'identified': 1,\n",
              "          'in': 4,\n",
              "          'international': 1,\n",
              "          'is': 1,\n",
              "          'january': 1,\n",
              "          'june': 1,\n",
              "          'known': 1,\n",
              "          'march': 1,\n",
              "          'million': 2,\n",
              "          'more': 4,\n",
              "          'of': 4,\n",
              "          'on': 2,\n",
              "          'ongoing': 1,\n",
              "          'organization': 1,\n",
              "          'outbreak': 2,\n",
              "          'pandemic': 4,\n",
              "          'people': 1,\n",
              "          'public': 1,\n",
              "          'recovered': 1,\n",
              "          'reported': 1,\n",
              "          'respiratory': 1,\n",
              "          'resulting': 1,\n",
              "          'sars‑cov‑2': 1,\n",
              "          'severe': 1,\n",
              "          'syndrome': 1,\n",
              "          'territories': 1,\n",
              "          'than': 4,\n",
              "          'the': 5,\n",
              "          'was': 1,\n",
              "          'world': 1,\n",
              "          'wuhan': 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLnMYI_aVNBM",
        "colab_type": "text"
      },
      "source": [
        "## Demo: Select x # of tokens of highest frequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jURqV2t_VTsK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "82fb0dad-5dac-46df-9e96-0d3fae9c556c"
      },
      "source": [
        "fdist_top10 = fdist.most_common(10)\n",
        "fdist_top10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 8),\n",
              " ('the', 5),\n",
              " ('pandemic', 4),\n",
              " ('of', 4),\n",
              " ('.', 4),\n",
              " ('in', 4),\n",
              " ('more', 4),\n",
              " ('than', 4),\n",
              " ('coronavirus', 3),\n",
              " ('covid-19', 2)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjKy4NDiVh2c",
        "colab_type": "text"
      },
      "source": [
        "## Demo: Blank Tokenizer \n",
        "- the blankline_tokenizer\n",
        "- indicates how many paragraphs there are"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzEpLQ6SVprp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "670a8ea9-6d63-4d74-e830-3fc1f1f5258f"
      },
      "source": [
        "from nltk.tokenize import blankline_tokenize\n",
        "rona_blank = blankline_tokenize(rona)\n",
        "len(rona_blank)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ0ujHKMWOAY",
        "colab_type": "text"
      },
      "source": [
        "## Bigrams, Trigrams & Ngrams\n",
        "**Bigrams** = tokens of 2 consecutive written words known as Bigrams\n",
        "\n",
        "**Trigrams** = tokes of 3 consecutive written words known as Trigram\n",
        "\n",
        "**Ngrams** = tokens of any number of consecutive written words known as Ngrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWEhOmpjWvSd",
        "colab_type": "text"
      },
      "source": [
        "### Demo: Bigrams, Trigrams & Ngrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtG_KSveWumX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "8bd477a3-2b82-4509-bcf5-98a3831b57ff"
      },
      "source": [
        "from nltk.util import bigrams, trigrams, ngrams\n",
        "string = \"The best and most beautiful things int he world canno tbe seen or even touched, they must be felt with the heart\"\n",
        "quote_tokens = nltk.word_tokenize(string)\n",
        "quotes_bigram = list(nltk.bigrams(quote_tokens))\n",
        "quotes_bigram"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'best'),\n",
              " ('best', 'and'),\n",
              " ('and', 'most'),\n",
              " ('most', 'beautiful'),\n",
              " ('beautiful', 'things'),\n",
              " ('things', 'int'),\n",
              " ('int', 'he'),\n",
              " ('he', 'world'),\n",
              " ('world', 'canno'),\n",
              " ('canno', 'tbe'),\n",
              " ('tbe', 'seen'),\n",
              " ('seen', 'or'),\n",
              " ('or', 'even'),\n",
              " ('even', 'touched'),\n",
              " ('touched', ','),\n",
              " (',', 'they'),\n",
              " ('they', 'must'),\n",
              " ('must', 'be'),\n",
              " ('be', 'felt'),\n",
              " ('felt', 'with'),\n",
              " ('with', 'the'),\n",
              " ('the', 'heart')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFUmIorVXdxe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "8d59845d-31aa-425a-98b8-6ed6a940fb64"
      },
      "source": [
        "quotes_trigrams = list(nltk.trigrams(quote_tokens))\n",
        "quotes_trigrams"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'best', 'and'),\n",
              " ('best', 'and', 'most'),\n",
              " ('and', 'most', 'beautiful'),\n",
              " ('most', 'beautiful', 'things'),\n",
              " ('beautiful', 'things', 'int'),\n",
              " ('things', 'int', 'he'),\n",
              " ('int', 'he', 'world'),\n",
              " ('he', 'world', 'canno'),\n",
              " ('world', 'canno', 'tbe'),\n",
              " ('canno', 'tbe', 'seen'),\n",
              " ('tbe', 'seen', 'or'),\n",
              " ('seen', 'or', 'even'),\n",
              " ('or', 'even', 'touched'),\n",
              " ('even', 'touched', ','),\n",
              " ('touched', ',', 'they'),\n",
              " (',', 'they', 'must'),\n",
              " ('they', 'must', 'be'),\n",
              " ('must', 'be', 'felt'),\n",
              " ('be', 'felt', 'with'),\n",
              " ('felt', 'with', 'the'),\n",
              " ('with', 'the', 'heart')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qctXClbcXn48",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "54c2f6b5-5a8d-48ab-d91e-c919519e7389"
      },
      "source": [
        "quotes_ngrams = list(nltk.ngrams(quote_tokens, 5))\n",
        "quotes_ngrams"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'best', 'and', 'most', 'beautiful'),\n",
              " ('best', 'and', 'most', 'beautiful', 'things'),\n",
              " ('and', 'most', 'beautiful', 'things', 'int'),\n",
              " ('most', 'beautiful', 'things', 'int', 'he'),\n",
              " ('beautiful', 'things', 'int', 'he', 'world'),\n",
              " ('things', 'int', 'he', 'world', 'canno'),\n",
              " ('int', 'he', 'world', 'canno', 'tbe'),\n",
              " ('he', 'world', 'canno', 'tbe', 'seen'),\n",
              " ('world', 'canno', 'tbe', 'seen', 'or'),\n",
              " ('canno', 'tbe', 'seen', 'or', 'even'),\n",
              " ('tbe', 'seen', 'or', 'even', 'touched'),\n",
              " ('seen', 'or', 'even', 'touched', ','),\n",
              " ('or', 'even', 'touched', ',', 'they'),\n",
              " ('even', 'touched', ',', 'they', 'must'),\n",
              " ('touched', ',', 'they', 'must', 'be'),\n",
              " (',', 'they', 'must', 'be', 'felt'),\n",
              " ('they', 'must', 'be', 'felt', 'with'),\n",
              " ('must', 'be', 'felt', 'with', 'the'),\n",
              " ('be', 'felt', 'with', 'the', 'heart')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL_uq5M5YioZ",
        "colab_type": "text"
      },
      "source": [
        "# Stemming\n",
        "- RECAP: normalize words into its root form\n",
        "- Literally just **cutting of the beginning or ending** of words whilst taking into account common prefixes & suffixes\n",
        "- This approach doesnt always work in finding the root word "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9HX0pb8ZE9u",
        "colab_type": "text"
      },
      "source": [
        "## PorterStem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEUHrDjWZAkq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f409731-e280-47a6-964a-20a16abfe495"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "pst = PorterStemmer()\n",
        "# find stemming of a word\n",
        "pst.stem('having')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'have'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mw00ZsMZp0b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "85bb5dc4-5c17-4830-9166-fb27df9709b5"
      },
      "source": [
        "words_to_stem=['give', \"giving\", \"given\", \"gave\"]\n",
        "for words in words_to_stem:\n",
        "  print(words + \":\" + pst.stem(words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "give:give\n",
            "giving:give\n",
            "given:given\n",
            "gave:gave\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2FAR9miZ_ss",
        "colab_type": "text"
      },
      "source": [
        "## Lancaster Stemmer\n",
        "- more aggressive than porter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu17UWOfaFFj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9aa2ea65-9328-4f2f-add7-d24f660b810c"
      },
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "lst=LancasterStemmer()\n",
        "for words in words_to_stem:\n",
        "  print(words + \":\" + lst.stem(words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "give:giv\n",
            "giving:giv\n",
            "given:giv\n",
            "gave:gav\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpUwTizcan-a",
        "colab_type": "text"
      },
      "source": [
        "# Lemmatization\n",
        "- Stemming but smarter by taking into account morphological analysis of words\n",
        "- need to have a detailed dictionary to link the word back to its **Lemma** = root word \n",
        "- Output is always a **proper word**\n",
        "  - unlike stemming, **giv** is not a word "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-2k4j1wbERR",
        "colab_type": "text"
      },
      "source": [
        "## Demo: Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edkrLYIubIzQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9a3e9b8a-4b56-4ca9-9ec7-d62f53a1a3be"
      },
      "source": [
        "from nltk.stem import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "# providing the dictionary\n",
        "word_lem = WordNetLemmatizer()\n",
        "\n",
        "for words in words_to_stem:\n",
        "  print(words + \":\" + word_lem.lemmatize(words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "give:give\n",
            "giving:giving\n",
            "given:given\n",
            "gave:gave\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clGgNBa2cB7R",
        "colab_type": "text"
      },
      "source": [
        "### Why did it not find the lemma?\n",
        "- we need to assign POS tags & hence **assumes all the words as nouns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mEgWItcb2LI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8699e3dd-3ed6-44c7-de2a-1c6214c8f73f"
      },
      "source": [
        "word_lem.lemmatize('corpora')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'corpus'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxuWtbtRcU-2",
        "colab_type": "text"
      },
      "source": [
        "# POS Tags\n",
        "- POS = Parts of Speech\n",
        "- tags what type of word it is: i.e. noun, verb etc. \n",
        "- a word can have more than one POS depending on the context its used in "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EwQ02jOcru-",
        "colab_type": "text"
      },
      "source": [
        "## Stop Words\n",
        "- words that doesnt not help with NLP, provides no meaning \n",
        "- ex. Really, All, Begin, Take, However, Of, The etc. \n",
        "- NLTK has a list of stop words\n",
        "- they are only helpful in the creation of sentences\n",
        "- they are not helpful in the processing of a language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACJtcwGwdIVa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f64dc83d-9d16-4e43-bf54-989d7da00c27"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x99Q3GsZdN91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "english_stopwords = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf6FfrvMdbHF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "571d4246-0c05-4bd2-8fee-5176290994b8"
      },
      "source": [
        "# we have 179 stop words in english\n",
        "len(english_stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwrM9CuJeqR2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "4c03452e-e72c-493d-b9e0-f808f3c0644b"
      },
      "source": [
        "# recap, the most frequent tokens \n",
        "fdist_top10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 8),\n",
              " ('the', 5),\n",
              " ('pandemic', 4),\n",
              " ('of', 4),\n",
              " ('.', 4),\n",
              " ('in', 4),\n",
              " ('more', 4),\n",
              " ('than', 4),\n",
              " ('coronavirus', 3),\n",
              " ('covid-19', 2)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTOkaEBTeykf",
        "colab_type": "text"
      },
      "source": [
        "## Removing punctuations & numbers as tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lb1NjHsex8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "punctuation = re.compile(r'[-.?!,:;()|0-9]')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl5SW3mSfMck",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b4effc9c-a28b-4d13-d5f6-29a695391430"
      },
      "source": [
        "post_punctuation=[]\n",
        "for words in rona_tokens:\n",
        "  word = punctuation.sub(\"\", words)\n",
        "  if len(word)>0:\n",
        "    post_punctuation.append(word)\n",
        "post_punctuation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'COVID',\n",
              " 'pandemic',\n",
              " 'also',\n",
              " 'known',\n",
              " 'as',\n",
              " 'the',\n",
              " 'coronavirus',\n",
              " 'pandemic',\n",
              " 'is',\n",
              " 'an',\n",
              " 'ongoing',\n",
              " 'pandemic',\n",
              " 'of',\n",
              " 'coronavirus',\n",
              " 'disease',\n",
              " 'COVID‑',\n",
              " 'caused',\n",
              " 'by',\n",
              " 'severe',\n",
              " 'acute',\n",
              " 'respiratory',\n",
              " 'syndrome',\n",
              " 'coronavirus',\n",
              " 'SARS‑CoV‑',\n",
              " 'The',\n",
              " 'outbreak',\n",
              " 'was',\n",
              " 'first',\n",
              " 'identified',\n",
              " 'in',\n",
              " 'Wuhan',\n",
              " 'China',\n",
              " 'in',\n",
              " 'December',\n",
              " 'The',\n",
              " 'World',\n",
              " 'Health',\n",
              " 'Organization',\n",
              " 'declared',\n",
              " 'the',\n",
              " 'outbreak',\n",
              " 'a',\n",
              " 'Public',\n",
              " 'Health',\n",
              " 'Emergency',\n",
              " 'of',\n",
              " 'International',\n",
              " 'Concern',\n",
              " 'on',\n",
              " 'January',\n",
              " 'and',\n",
              " 'a',\n",
              " 'pandemic',\n",
              " 'on',\n",
              " 'March',\n",
              " 'As',\n",
              " 'of',\n",
              " 'June',\n",
              " 'more',\n",
              " 'than',\n",
              " 'million',\n",
              " 'cases',\n",
              " 'of',\n",
              " 'COVID',\n",
              " 'have',\n",
              " 'been',\n",
              " 'reported',\n",
              " 'in',\n",
              " 'more',\n",
              " 'than',\n",
              " 'countries',\n",
              " 'and',\n",
              " 'territories',\n",
              " 'resulting',\n",
              " 'in',\n",
              " 'more',\n",
              " 'than',\n",
              " 'deaths',\n",
              " 'more',\n",
              " 'than',\n",
              " 'million',\n",
              " 'people',\n",
              " 'have',\n",
              " 'recovered']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1bYbBzAglYZ",
        "colab_type": "text"
      },
      "source": [
        "## POS Tags List \n",
        "![alt text](https://thottingal.in/wp-content/uploads/2019/09/PENN-treebank-tagset.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyG-UtEChaCw",
        "colab_type": "text"
      },
      "source": [
        "## Demo\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhTmyrnAhadm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "08611a2f-3e4c-4334-e996-870584cc0c6a"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sentence = \"Timothy is a natural when it comes to drawing\"\n",
        "sentence_tokens = word_tokenize(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2AtuYUbhqVb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "9b56431a-6cac-475c-8f65-233a9df79115"
      },
      "source": [
        "for token in sentence_tokens:\n",
        "  print(nltk.pos_tag([token]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Timothy', 'NN')]\n",
            "[('is', 'VBZ')]\n",
            "[('a', 'DT')]\n",
            "[('natural', 'JJ')]\n",
            "[('when', 'WRB')]\n",
            "[('it', 'PRP')]\n",
            "[('comes', 'VBZ')]\n",
            "[('to', 'TO')]\n",
            "[('drawing', 'VBG')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiktWjOniFT6",
        "colab_type": "text"
      },
      "source": [
        "# Named Entity Recognition \n",
        "- Movie, Monetary Value, Organization, Location, Quantities, Person\n",
        "\n",
        "## 3 types of identification \n",
        "1. **non phrase identification** = extracting non phrase using dependency passing & POS tagging\n",
        "2. **phrase classification** = the non phrases are categorized into location, movies etc. \n",
        "3. **knowledge graphs** =  validation layer for when phrases are misclassified \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX8M4R-tjawA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e0a1092d-0348-41a0-93bf-d3ec470bb789"
      },
      "source": [
        "from nltk import ne_chunk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSlObs_MjhvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NE_sentence = \"The US President stays in the WHITE HOUSE\"\n",
        "NE_tokens = word_tokenize(NE_sentence)\n",
        "NE_tags = nltk.pos_tag(NE_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZSQg6pdjz0M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "bd6ca464-d901-4db2-a64c-2fed570ed6af"
      },
      "source": [
        "NE_NER = ne_chunk(NE_tags)\n",
        "print(NE_NER)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  The/DT\n",
            "  (ORGANIZATION US/NNP)\n",
            "  President/NNP\n",
            "  stays/VBZ\n",
            "  in/IN\n",
            "  the/DT\n",
            "  (FACILITY WHITE/NNP HOUSE/NNP))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVYsfcMnkQvm",
        "colab_type": "text"
      },
      "source": [
        "# Syntax \n",
        "Linguistics Definition = set of rules, principles, and processes within a given sentence within a given language "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEzrlMROkheW",
        "colab_type": "text"
      },
      "source": [
        "## Phrase Structure Rules \n",
        "- phrase structure rules specify the well-formes structures of sentences\n",
        "- a tree must match the phrase structure rules to be grammatical\n",
        "![alt text](https://image.slidesharecdn.com/recursion-140717094159-phpapp01/95/recursion-16-638.jpg?cb=1405590175)\n",
        "- NP = noun phrase\n",
        "- VP = verb phrase\n",
        "- PP - prepositional phrase \n",
        "- P = proposition\n",
        "- Art = article \n",
        "\n",
        "\n",
        "##Download **GhostScript**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ij6pQDXm__Y",
        "colab_type": "text"
      },
      "source": [
        "#Chunking \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXGb3WGsnMYg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "98d82ba9-54c0-453f-b3bc-d13c8cc694c0"
      },
      "source": [
        "new = \"The big cat ate the little mouse who was after fresh cheese\"\n",
        "new_tokens = nltk.pos_tag(word_tokenize(new))\n",
        "new_tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('big', 'JJ'),\n",
              " ('cat', 'NN'),\n",
              " ('ate', 'VBD'),\n",
              " ('the', 'DT'),\n",
              " ('little', 'JJ'),\n",
              " ('mouse', 'NN'),\n",
              " ('who', 'WP'),\n",
              " ('was', 'VBD'),\n",
              " ('after', 'IN'),\n",
              " ('fresh', 'JJ'),\n",
              " ('cheese', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ulyi_5cjns5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#specifying the grammer for a noun phrase\n",
        "grammar_np = r\"NP: {<DT>?<JJ><NN>}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXZ1pfnin9gn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# regex matching screen\n",
        "chunk_parser = nltk.RegexpParser(grammar_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqzMkX7JoNdv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "a7c8cc98-a092-4ab6-e90c-4a65dd38abf6"
      },
      "source": [
        "chunk_result = chunk_parser.parse(new_tokens)\n",
        "chunk_result\n",
        "\n",
        "#error occurs bc we didnt use ghost script & syntax tree \n",
        "# but you still see:\n",
        "#Tree('S', [\n",
        "  #Tree('NP', [('The', 'DT'), ('big', 'JJ'), ('cat', 'NN')]), ('ate', 'VBD'), \n",
        "  #Tree('NP', [('the', 'DT'), ('little', 'JJ'), ('mouse', 'NN')]), ('who', 'WP'), ('was', 'VBD'), ('after', 'IN'), \n",
        "  #Tree('NP', [('fresh', 'JJ'), ('cheese', 'NN')])\n",
        "#])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_binary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0m_canvas_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0mwidget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_to_treesegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_widget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/draw/util.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parent, **kw)\u001b[0m\n\u001b[1;32m   1651\u001b[0m         \u001b[0;31m# If no parent was given, set up a top-level window.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NLTK'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<Control-p>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2021\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2023\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2024\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('NP', [('The', 'DT'), ('big', 'JJ'), ('cat', 'NN')]), ('ate', 'VBD'), Tree('NP', [('the', 'DT'), ('little', 'JJ'), ('mouse', 'NN')]), ('who', 'WP'), ('was', 'VBD'), ('after', 'IN'), Tree('NP', [('fresh', 'JJ'), ('cheese', 'NN')])])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkBPzyCtpMzk",
        "colab_type": "text"
      },
      "source": [
        "# Summary Project: ML Classifier on Movie Reviews from NLTK Corpora"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgBFaY3xpTES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTnhZ8u1p1--",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7d1ddddc-2d84-4f93-9c32-7246cb1fcaac"
      },
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "nltk.download('movie_reviews')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiGazIXPplyq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "497db7ef-4c71-40ba-dc9e-413930874a98"
      },
      "source": [
        "print(movie_reviews.categories())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['neg', 'pos']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqJyQiqlqZW7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "218be00c-2f80-4e14-bb21-6b7c3349875a"
      },
      "source": [
        "review = nltk.corpus.movie_reviews.words('pos/cv000_29590.txt')\n",
        "review"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['films', 'adapted', 'from', 'comic', 'books', 'have', ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8YKCAHfr-17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review_list = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_AEHEexqrpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_reviews = movie_reviews.fileids('pos')\n",
        "for rev in pos_reviews:\n",
        "  rev_text_pos = rev = nltk.corpus.movie_reviews.words(rev)\n",
        "  review_one_string = \" \".join(rev_text_pos)\n",
        "  review_one_string = review_one_string.replace(' ,', ',')\n",
        "  review_one_string = review_one_string.replace(' .', '.')\n",
        "  review_one_string = review_one_string.replace(\"\\' \", \"'\")\n",
        "  review_one_string = review_one_string.replace(\" \\'\", \"'\")\n",
        "  review_list.append(review_one_string)\n",
        "\n",
        "neg_reviews = movie_reviews.fileids('neg')\n",
        "for rev in neg_reviews:\n",
        "  rev_text_neg = rev = nltk.corpus.movie_reviews.words(rev)\n",
        "  review_one_string = \" \".join(rev_text_pos)\n",
        "  review_one_string = review_one_string.replace(' ,', ',')\n",
        "  review_one_string = review_one_string.replace(' .', '.')\n",
        "  review_one_string = review_one_string.replace(\"\\' \", \"'\")\n",
        "  review_one_string = review_one_string.replace(\" \\'\", \"'\")\n",
        "  review_list.append(review_one_string)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kL5HCUJ-sRqi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26c686c5-5712-4f32-f506-bb42376fc53f"
      },
      "source": [
        "len(review_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHwmMq2MsaX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create targets \n",
        "neg_targets = np.zeros((1000,), dtype=np.int)\n",
        "pos_targets = np.ones((1000,), dtype=np.int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sHHrwG7syjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_list = []\n",
        "for neg_tar in neg_targets:\n",
        "  target_list.append(neg_tar)\n",
        "for pos_tar in pos_targets:\n",
        "  target_list.append(pos_tar)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zssysR7s9bH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b580cd8a-6859-431b-8ad6-ce9fdb7a6c3e"
      },
      "source": [
        "len(target_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Upx-Nn3mtBD5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "6620d031-35c9-4355-9e7b-60529f04cf0b"
      },
      "source": [
        "y = pd.Series(target_list)\n",
        "type(y)\n",
        "y.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0\n",
              "1    0\n",
              "2    0\n",
              "3    0\n",
              "4    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RMYh_cBtZfz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "526af2e6-cfca-41e7-c87a-e0e8667593b8"
      },
      "source": [
        "# now start create features with countvectorizor \n",
        "\n",
        "# init count vectorizor \n",
        "count_vect = CountVectorizer(lowercase=True, stop_words='english', min_df =2)\n",
        "\n",
        "X_count_vect = count_vect.fit_transform(review_list)\n",
        "\n",
        "X_count_vect.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 17316)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASgG7D_ouH51",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5708c3af-6382-47c8-d06a-e51f877cfd5f"
      },
      "source": [
        "X_names = count_vect.get_feature_names()\n",
        "X_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['00',\n",
              " '000',\n",
              " '007',\n",
              " '10',\n",
              " '100',\n",
              " '1000',\n",
              " '101',\n",
              " '102',\n",
              " '105',\n",
              " '107',\n",
              " '11',\n",
              " '110',\n",
              " '12',\n",
              " '129',\n",
              " '13',\n",
              " '130',\n",
              " '137',\n",
              " '13th',\n",
              " '14',\n",
              " '14th',\n",
              " '15',\n",
              " '150',\n",
              " '1500s',\n",
              " '155',\n",
              " '16',\n",
              " '160',\n",
              " '161',\n",
              " '16mm',\n",
              " '16th',\n",
              " '16x9',\n",
              " '17',\n",
              " '175',\n",
              " '1773',\n",
              " '17th',\n",
              " '18',\n",
              " '180',\n",
              " '1800s',\n",
              " '1839',\n",
              " '1888',\n",
              " '18th',\n",
              " '19',\n",
              " '1900',\n",
              " '1912',\n",
              " '1914',\n",
              " '1919',\n",
              " '1930',\n",
              " '1930s',\n",
              " '1932',\n",
              " '1935',\n",
              " '1937',\n",
              " '1938',\n",
              " '1939',\n",
              " '1940',\n",
              " '1940s',\n",
              " '1941',\n",
              " '1943',\n",
              " '1944',\n",
              " '1945',\n",
              " '1947',\n",
              " '1950',\n",
              " '1950s',\n",
              " '1953',\n",
              " '1954',\n",
              " '1957',\n",
              " '1958',\n",
              " '1959',\n",
              " '1960',\n",
              " '1960s',\n",
              " '1962',\n",
              " '1963',\n",
              " '1964',\n",
              " '1965',\n",
              " '1967',\n",
              " '1968',\n",
              " '1969',\n",
              " '1970',\n",
              " '1970s',\n",
              " '1971',\n",
              " '1972',\n",
              " '1973',\n",
              " '1974',\n",
              " '1975',\n",
              " '1976',\n",
              " '1977',\n",
              " '1978',\n",
              " '1979',\n",
              " '1980',\n",
              " '1980s',\n",
              " '1981',\n",
              " '1983',\n",
              " '1984',\n",
              " '1985',\n",
              " '1986',\n",
              " '1987',\n",
              " '1988',\n",
              " '1989',\n",
              " '1990',\n",
              " '1990s',\n",
              " '1991',\n",
              " '1992',\n",
              " '1993',\n",
              " '1994',\n",
              " '1995',\n",
              " '1996',\n",
              " '1997',\n",
              " '1998',\n",
              " '1999',\n",
              " '19th',\n",
              " '1st',\n",
              " '20',\n",
              " '200',\n",
              " '2000',\n",
              " '2001',\n",
              " '2013',\n",
              " '2050',\n",
              " '20th',\n",
              " '21',\n",
              " '21st',\n",
              " '22',\n",
              " '23',\n",
              " '24',\n",
              " '2400',\n",
              " '24th',\n",
              " '25',\n",
              " '250',\n",
              " '25th',\n",
              " '26',\n",
              " '27',\n",
              " '28',\n",
              " '29',\n",
              " '2nd',\n",
              " '30',\n",
              " '300',\n",
              " '31',\n",
              " '33',\n",
              " '35',\n",
              " '35mm',\n",
              " '36',\n",
              " '37',\n",
              " '3d',\n",
              " '3po',\n",
              " '3rd',\n",
              " '40',\n",
              " '40s',\n",
              " '41',\n",
              " '42',\n",
              " '44',\n",
              " '45',\n",
              " '48',\n",
              " '4th',\n",
              " '50',\n",
              " '500',\n",
              " '50s',\n",
              " '52',\n",
              " '53',\n",
              " '54',\n",
              " '56',\n",
              " '57',\n",
              " '571',\n",
              " '5th',\n",
              " '60',\n",
              " '600',\n",
              " '60s',\n",
              " '61',\n",
              " '63',\n",
              " '65',\n",
              " '66',\n",
              " '69',\n",
              " '70',\n",
              " '700',\n",
              " '70mm',\n",
              " '70s',\n",
              " '73',\n",
              " '75',\n",
              " '77',\n",
              " '7th',\n",
              " '80',\n",
              " '80s',\n",
              " '81',\n",
              " '85',\n",
              " '88',\n",
              " '8mm',\n",
              " '90',\n",
              " '90210',\n",
              " '90s',\n",
              " '91',\n",
              " '911',\n",
              " '92',\n",
              " '93',\n",
              " '94',\n",
              " '95',\n",
              " '96',\n",
              " '97',\n",
              " '98',\n",
              " '99',\n",
              " '_film',\n",
              " '_is_',\n",
              " '_life',\n",
              " '_not_',\n",
              " '_that_',\n",
              " '_the',\n",
              " 'aardman',\n",
              " 'aaron',\n",
              " 'abandon',\n",
              " 'abandoned',\n",
              " 'abandonment',\n",
              " 'abandons',\n",
              " 'abby',\n",
              " 'abc',\n",
              " 'abducted',\n",
              " 'abduction',\n",
              " 'abel',\n",
              " 'abetted',\n",
              " 'abilities',\n",
              " 'ability',\n",
              " 'able',\n",
              " 'ably',\n",
              " 'aboard',\n",
              " 'abode',\n",
              " 'abolish',\n",
              " 'abolitionists',\n",
              " 'abortion',\n",
              " 'abortions',\n",
              " 'abound',\n",
              " 'abounds',\n",
              " 'abraham',\n",
              " 'abrahams',\n",
              " 'abrasive',\n",
              " 'abroad',\n",
              " 'abrupt',\n",
              " 'abruptly',\n",
              " 'absence',\n",
              " 'absent',\n",
              " 'absolute',\n",
              " 'absolutely',\n",
              " 'absorb',\n",
              " 'absorbed',\n",
              " 'absorbing',\n",
              " 'absorption',\n",
              " 'abstinence',\n",
              " 'abstract',\n",
              " 'absurd',\n",
              " 'absurdist',\n",
              " 'absurdities',\n",
              " 'absurdity',\n",
              " 'absurdly',\n",
              " 'abundance',\n",
              " 'abundant',\n",
              " 'abundantly',\n",
              " 'abuse',\n",
              " 'abused',\n",
              " 'abusers',\n",
              " 'abusive',\n",
              " 'abyss',\n",
              " 'academia',\n",
              " 'academic',\n",
              " 'academy',\n",
              " 'accent',\n",
              " 'accents',\n",
              " 'accentuate',\n",
              " 'accentuates',\n",
              " 'accept',\n",
              " 'acceptable',\n",
              " 'acceptance',\n",
              " 'accepted',\n",
              " 'accepting',\n",
              " 'accepts',\n",
              " 'access',\n",
              " 'accessibility',\n",
              " 'accessible',\n",
              " 'accident',\n",
              " 'accidental',\n",
              " 'accidentally',\n",
              " 'accidents',\n",
              " 'acclaim',\n",
              " 'acclaimed',\n",
              " 'accolades',\n",
              " 'accommodating',\n",
              " 'accompanied',\n",
              " 'accompanies',\n",
              " 'accompaniment',\n",
              " 'accompany',\n",
              " 'accompanying',\n",
              " 'accomplice',\n",
              " 'accomplish',\n",
              " 'accomplished',\n",
              " 'accomplishes',\n",
              " 'accomplishment',\n",
              " 'accomplishments',\n",
              " 'accordance',\n",
              " 'according',\n",
              " 'accordingly',\n",
              " 'accordion',\n",
              " 'account',\n",
              " 'accountant',\n",
              " 'accounts',\n",
              " 'accumulated',\n",
              " 'accuracy',\n",
              " 'accurate',\n",
              " 'accurately',\n",
              " 'accusation',\n",
              " 'accusations',\n",
              " 'accuse',\n",
              " 'accused',\n",
              " 'accuses',\n",
              " 'accustomed',\n",
              " 'ace',\n",
              " 'acerbic',\n",
              " 'achieve',\n",
              " 'achieved',\n",
              " 'achievement',\n",
              " 'achievements',\n",
              " 'achieves',\n",
              " 'achieving',\n",
              " 'achingly',\n",
              " 'acid',\n",
              " 'acidic',\n",
              " 'ack',\n",
              " 'acknowledge',\n",
              " 'acknowledged',\n",
              " 'acknowledges',\n",
              " 'acknowledging',\n",
              " 'acknowledgment',\n",
              " 'acquaintance',\n",
              " 'acquaintances',\n",
              " 'acquired',\n",
              " 'acquit',\n",
              " 'acquits',\n",
              " 'act',\n",
              " 'acted',\n",
              " 'acting',\n",
              " 'action',\n",
              " 'actioner',\n",
              " 'actions',\n",
              " 'active',\n",
              " 'actively',\n",
              " 'activist',\n",
              " 'activists',\n",
              " 'activities',\n",
              " 'activity',\n",
              " 'actor',\n",
              " 'actors',\n",
              " 'actress',\n",
              " 'actresses',\n",
              " 'acts',\n",
              " 'actual',\n",
              " 'actuality',\n",
              " 'actualization',\n",
              " 'actually',\n",
              " 'acutely',\n",
              " 'ad',\n",
              " 'ad2am',\n",
              " 'adage',\n",
              " 'adam',\n",
              " 'adams',\n",
              " 'adapt',\n",
              " 'adaptation',\n",
              " 'adaptations',\n",
              " 'adapted',\n",
              " 'adapting',\n",
              " 'adaption',\n",
              " 'add',\n",
              " 'addams',\n",
              " 'added',\n",
              " 'addict',\n",
              " 'addicted',\n",
              " 'addiction',\n",
              " 'adding',\n",
              " 'addition',\n",
              " 'additional',\n",
              " 'additionally',\n",
              " 'addled',\n",
              " 'address',\n",
              " 'addressed',\n",
              " 'addresses',\n",
              " 'addressing',\n",
              " 'adds',\n",
              " 'addy',\n",
              " 'ade',\n",
              " 'adefarasin',\n",
              " 'adept',\n",
              " 'adequate',\n",
              " 'adequately',\n",
              " 'adheres',\n",
              " 'adjective',\n",
              " 'adjectives',\n",
              " 'adjoining',\n",
              " 'adjust',\n",
              " 'admirable',\n",
              " 'admirably',\n",
              " 'admiration',\n",
              " 'admire',\n",
              " 'admired',\n",
              " 'admirer',\n",
              " 'admires',\n",
              " 'admission',\n",
              " 'admit',\n",
              " 'admits',\n",
              " 'admittance',\n",
              " 'admitted',\n",
              " 'admittedly',\n",
              " 'admitting',\n",
              " 'admittingly',\n",
              " 'admonition',\n",
              " 'adolescence',\n",
              " 'adolescent',\n",
              " 'adolescents',\n",
              " 'adopt',\n",
              " 'adopted',\n",
              " 'adopting',\n",
              " 'adoption',\n",
              " 'adopts',\n",
              " 'adorable',\n",
              " 'adoration',\n",
              " 'adore',\n",
              " 'adored',\n",
              " 'adrenalin',\n",
              " 'adrenaline',\n",
              " 'adrian',\n",
              " 'adrien',\n",
              " 'adrift',\n",
              " 'adroitly',\n",
              " 'ads',\n",
              " 'adult',\n",
              " 'adulterous',\n",
              " 'adultery',\n",
              " 'adulthood',\n",
              " 'adults',\n",
              " 'advance',\n",
              " 'advanced',\n",
              " 'advancement',\n",
              " 'advances',\n",
              " 'advancing',\n",
              " 'advantage',\n",
              " 'advantages',\n",
              " 'advent',\n",
              " 'adventure',\n",
              " 'adventurer',\n",
              " 'adventures',\n",
              " 'adventurous',\n",
              " 'adversarial',\n",
              " 'adversary',\n",
              " 'adverse',\n",
              " 'adversity',\n",
              " 'advertised',\n",
              " 'advertisement',\n",
              " 'advertisements',\n",
              " 'advertising',\n",
              " 'advice',\n",
              " 'advisable',\n",
              " 'advised',\n",
              " 'adviser',\n",
              " 'advisers',\n",
              " 'advises',\n",
              " 'advising',\n",
              " 'advisor',\n",
              " 'advisors',\n",
              " 'advocate',\n",
              " 'advocating',\n",
              " 'aerial',\n",
              " 'aesthetic',\n",
              " 'aesthetically',\n",
              " 'aesthetics',\n",
              " 'afar',\n",
              " 'affable',\n",
              " 'affair',\n",
              " 'affairs',\n",
              " 'affect',\n",
              " 'affectations',\n",
              " 'affected',\n",
              " 'affecting',\n",
              " 'affection',\n",
              " 'affectionate',\n",
              " 'affectionately',\n",
              " 'affections',\n",
              " 'affects',\n",
              " 'affiliate',\n",
              " 'affinity',\n",
              " 'affirmation',\n",
              " 'affirmative',\n",
              " 'affirming',\n",
              " 'affleck',\n",
              " 'afflicted',\n",
              " 'affliction',\n",
              " 'afford',\n",
              " 'affordable',\n",
              " 'afforded',\n",
              " 'affraid',\n",
              " 'aficionados',\n",
              " 'afloat',\n",
              " 'afoot',\n",
              " 'afore',\n",
              " 'aforementioned',\n",
              " 'afraid',\n",
              " 'africa',\n",
              " 'african',\n",
              " 'africans',\n",
              " 'aftereffects',\n",
              " 'afterglow',\n",
              " 'afterlife',\n",
              " 'aftermath',\n",
              " 'afternoon',\n",
              " 'aftertaste',\n",
              " 'afterthought',\n",
              " 'afterward',\n",
              " 'agatha',\n",
              " 'age',\n",
              " 'aged',\n",
              " 'ageing',\n",
              " 'agency',\n",
              " 'agenda',\n",
              " 'agent',\n",
              " 'agents',\n",
              " 'ages',\n",
              " 'aggression',\n",
              " 'aggressive',\n",
              " 'aggressively',\n",
              " 'agile',\n",
              " 'aging',\n",
              " 'ago',\n",
              " 'agonizing',\n",
              " 'agree',\n",
              " 'agreeable',\n",
              " 'agreed',\n",
              " 'agreeing',\n",
              " 'agreement',\n",
              " 'agrees',\n",
              " 'ah',\n",
              " 'ahead',\n",
              " 'ahmed',\n",
              " 'aid',\n",
              " 'aidan',\n",
              " 'aide',\n",
              " 'aided',\n",
              " 'aiding',\n",
              " 'aids',\n",
              " 'ailing',\n",
              " 'aim',\n",
              " 'aimed',\n",
              " 'aimee',\n",
              " 'aiming',\n",
              " 'aimless',\n",
              " 'aimlessly',\n",
              " 'aims',\n",
              " 'ain',\n",
              " 'air',\n",
              " 'aircraft',\n",
              " 'aired',\n",
              " 'aires',\n",
              " 'airline',\n",
              " 'airplane',\n",
              " 'airport',\n",
              " 'airwaves',\n",
              " 'airy',\n",
              " 'aisle',\n",
              " 'aka',\n",
              " 'aki',\n",
              " 'akin',\n",
              " 'akira',\n",
              " 'al',\n",
              " 'ala',\n",
              " 'aladdin',\n",
              " 'alain',\n",
              " 'alan',\n",
              " 'alanis',\n",
              " 'alarm',\n",
              " 'alas',\n",
              " 'albania',\n",
              " 'albanian',\n",
              " 'albeit',\n",
              " 'albert',\n",
              " 'alberta',\n",
              " 'albertson',\n",
              " 'album',\n",
              " 'albums',\n",
              " 'alcohol',\n",
              " 'alcoholic',\n",
              " 'alcoholism',\n",
              " 'alda',\n",
              " 'alec',\n",
              " 'alejandro',\n",
              " 'alek',\n",
              " 'alert',\n",
              " 'alessandro',\n",
              " 'alex',\n",
              " 'alexander',\n",
              " 'alexandra',\n",
              " 'alfre',\n",
              " 'alfred',\n",
              " 'algar',\n",
              " 'ali',\n",
              " 'aliases',\n",
              " 'alice',\n",
              " 'alicia',\n",
              " 'alida',\n",
              " 'alien',\n",
              " 'alienate',\n",
              " 'alienated',\n",
              " 'alienates',\n",
              " 'alienating',\n",
              " 'alienation',\n",
              " 'aliens',\n",
              " 'alike',\n",
              " 'alison',\n",
              " 'alive',\n",
              " 'allah',\n",
              " 'allan',\n",
              " 'allegations',\n",
              " 'alleged',\n",
              " 'allegedly',\n",
              " 'allegiances',\n",
              " 'allegory',\n",
              " 'allegra',\n",
              " 'allen',\n",
              " 'alley',\n",
              " 'alleys',\n",
              " 'alliance',\n",
              " 'allied',\n",
              " 'allies',\n",
              " 'alligators',\n",
              " 'allison',\n",
              " 'allow',\n",
              " 'allowed',\n",
              " 'allowing',\n",
              " 'allows',\n",
              " 'allure',\n",
              " 'alluring',\n",
              " 'allusions',\n",
              " 'ally',\n",
              " 'alma',\n",
              " 'alongside',\n",
              " 'aloof',\n",
              " 'alot',\n",
              " 'aloud',\n",
              " 'alright',\n",
              " 'altar',\n",
              " 'alter',\n",
              " 'alteration',\n",
              " 'alterations',\n",
              " 'altered',\n",
              " 'altering',\n",
              " 'alternate',\n",
              " 'alternately',\n",
              " 'alternates',\n",
              " 'alternating',\n",
              " 'alternative',\n",
              " 'alters',\n",
              " 'althea',\n",
              " 'altman',\n",
              " 'altogether',\n",
              " 'altough',\n",
              " 'alum',\n",
              " 'alumnus',\n",
              " 'alvarado',\n",
              " 'alzheimer',\n",
              " 'amadeus',\n",
              " 'amanda',\n",
              " 'amateurish',\n",
              " 'amaze',\n",
              " 'amazed',\n",
              " 'amazement',\n",
              " 'amazes',\n",
              " 'amazing',\n",
              " 'amazingly',\n",
              " 'ambassador',\n",
              " 'ambassadors',\n",
              " 'amber',\n",
              " 'ambiance',\n",
              " 'ambient',\n",
              " 'ambiguities',\n",
              " 'ambiguity',\n",
              " 'ambiguous',\n",
              " 'ambiguously',\n",
              " 'ambition',\n",
              " 'ambitions',\n",
              " 'ambitious',\n",
              " 'ambivalence',\n",
              " 'ambivalent',\n",
              " 'ambulance',\n",
              " 'amendment',\n",
              " 'amends',\n",
              " 'america',\n",
              " 'american',\n",
              " 'americanized',\n",
              " 'americans',\n",
              " 'americas',\n",
              " 'ames',\n",
              " 'amiable',\n",
              " 'amid',\n",
              " 'amidala',\n",
              " 'amidst',\n",
              " 'amiss',\n",
              " 'amistad',\n",
              " 'amnesia',\n",
              " 'amnesiac',\n",
              " 'amon',\n",
              " 'amoral',\n",
              " 'amos',\n",
              " 'amounts',\n",
              " 'amphibian',\n",
              " 'ample',\n",
              " 'amplified',\n",
              " 'amuse',\n",
              " 'amused',\n",
              " 'amusement',\n",
              " 'amusing',\n",
              " 'amusingly',\n",
              " 'amy',\n",
              " 'anaconda',\n",
              " 'anakin',\n",
              " 'anal',\n",
              " 'analogy',\n",
              " 'analyses',\n",
              " 'analysis',\n",
              " 'analyst',\n",
              " 'analysts',\n",
              " 'analyze',\n",
              " 'analyzed',\n",
              " 'analyzing',\n",
              " 'anand',\n",
              " 'anarchic',\n",
              " 'anarchists',\n",
              " 'anarchy',\n",
              " 'anastasia',\n",
              " 'anatomy',\n",
              " 'ancestors',\n",
              " 'anchor',\n",
              " 'ancient',\n",
              " 'anderson',\n",
              " 'andie',\n",
              " 'andre',\n",
              " 'andrea',\n",
              " 'andreas',\n",
              " 'andrew',\n",
              " 'android',\n",
              " 'andromeda',\n",
              " 'andy',\n",
              " 'anecdote',\n",
              " 'anecdotes',\n",
              " 'anew',\n",
              " 'ang',\n",
              " 'angel',\n",
              " 'angela',\n",
              " 'angeles',\n",
              " 'angelic',\n",
              " 'angelina',\n",
              " 'angelo',\n",
              " 'angels',\n",
              " 'anger',\n",
              " 'angered',\n",
              " 'angers',\n",
              " 'anglade',\n",
              " 'angle',\n",
              " 'angles',\n",
              " 'angry',\n",
              " 'angst',\n",
              " 'anguish',\n",
              " 'anguished',\n",
              " 'angus',\n",
              " 'anh',\n",
              " 'animal',\n",
              " 'animals',\n",
              " 'animate',\n",
              " 'animated',\n",
              " 'animation',\n",
              " 'animations',\n",
              " 'animator',\n",
              " 'animators',\n",
              " 'animatronic',\n",
              " 'anime',\n",
              " 'animosity',\n",
              " 'aniston',\n",
              " 'anita',\n",
              " 'anjelica',\n",
              " 'ann',\n",
              " 'anna',\n",
              " 'annabella',\n",
              " 'anne',\n",
              " 'annette',\n",
              " 'annie',\n",
              " 'annihilation',\n",
              " 'anniversary',\n",
              " 'announce',\n",
              " 'announced',\n",
              " 'announcement',\n",
              " 'announcer',\n",
              " 'announces',\n",
              " 'announcing',\n",
              " 'annoy',\n",
              " 'annoyance',\n",
              " 'annoyed',\n",
              " 'annoying',\n",
              " 'annual',\n",
              " 'anonymity',\n",
              " 'anonymous',\n",
              " 'anonymously',\n",
              " 'ansell',\n",
              " 'answer',\n",
              " 'answered',\n",
              " 'answers',\n",
              " 'ant',\n",
              " 'antagonist',\n",
              " 'antagonists',\n",
              " 'antarctica',\n",
              " 'ante',\n",
              " 'antenna',\n",
              " 'anthony',\n",
              " 'anti',\n",
              " 'anticipate',\n",
              " 'anticipated',\n",
              " 'anticipating',\n",
              " 'anticipation',\n",
              " 'anticlimactic',\n",
              " 'antics',\n",
              " 'antidote',\n",
              " 'antique',\n",
              " 'antithesis',\n",
              " 'antoine',\n",
              " 'anton',\n",
              " 'antonio',\n",
              " 'ants',\n",
              " 'antz',\n",
              " 'anxiety',\n",
              " 'anxious',\n",
              " 'anxiously',\n",
              " 'anybody',\n",
              " 'anymore',\n",
              " 'anytime',\n",
              " 'aoki',\n",
              " 'ap2',\n",
              " 'apart',\n",
              " 'apartment',\n",
              " 'apartments',\n",
              " 'apathy',\n",
              " 'ape',\n",
              " 'apes',\n",
              " 'aphrodite',\n",
              " 'aplomb',\n",
              " 'apocalypse',\n",
              " 'apocalyptic',\n",
              " 'apollo',\n",
              " 'apologies',\n",
              " 'apologize',\n",
              " 'apologizing',\n",
              " 'apostle',\n",
              " 'appalling',\n",
              " 'apparatus',\n",
              " 'apparent',\n",
              " 'apparently',\n",
              " 'apparition',\n",
              " 'apparitions',\n",
              " 'appeal',\n",
              " 'appealing',\n",
              " 'appeals',\n",
              " 'appear',\n",
              " 'appearance',\n",
              " 'appearances',\n",
              " 'appeared',\n",
              " 'appearing',\n",
              " 'appears',\n",
              " 'apperance',\n",
              " 'appetite',\n",
              " 'appetites',\n",
              " 'appetizing',\n",
              " 'applaud',\n",
              " 'applauded',\n",
              " 'applause',\n",
              " 'apple',\n",
              " 'appliances',\n",
              " 'applicable',\n",
              " 'application',\n",
              " 'applications',\n",
              " 'applied',\n",
              " 'applies',\n",
              " 'apply',\n",
              " 'appointed',\n",
              " 'appointment',\n",
              " 'appreciate',\n",
              " 'appreciated',\n",
              " 'appreciating',\n",
              " 'appreciation',\n",
              " 'apprehended',\n",
              " 'apprehension',\n",
              " 'apprehensions',\n",
              " 'apprentice',\n",
              " 'approach',\n",
              " 'approached',\n",
              " 'approaches',\n",
              " 'approaching',\n",
              " 'appropriate',\n",
              " 'appropriately',\n",
              " 'approve',\n",
              " 'approximately',\n",
              " 'april',\n",
              " 'apt',\n",
              " 'aptitude',\n",
              " 'aptly',\n",
              " 'aquarium',\n",
              " 'arab',\n",
              " 'arabia',\n",
              " 'arabian',\n",
              " 'arabs',\n",
              " 'arachnid',\n",
              " 'arachnids',\n",
              " 'arbitrary',\n",
              " 'arc',\n",
              " 'arch',\n",
              " 'archbishop',\n",
              " 'archer',\n",
              " 'archetypal',\n",
              " 'archetypes',\n",
              " 'archie',\n",
              " 'architect',\n",
              " 'architecture',\n",
              " 'archival',\n",
              " 'arctic',\n",
              " 'ardent',\n",
              " 'ardently',\n",
              " 'area',\n",
              " 'areas',\n",
              " 'aren',\n",
              " 'arena',\n",
              " 'argento',\n",
              " 'arguably',\n",
              " 'argue',\n",
              " 'argued',\n",
              " 'argues',\n",
              " 'arguing',\n",
              " 'argument',\n",
              " 'arguments',\n",
              " 'aria',\n",
              " 'arid',\n",
              " 'ariel',\n",
              " 'arise',\n",
              " 'arises',\n",
              " 'aristocracy',\n",
              " 'aristocrat',\n",
              " 'aristocratic',\n",
              " 'aristocrats',\n",
              " 'arizona',\n",
              " 'ark',\n",
              " 'arkin',\n",
              " 'arlene',\n",
              " 'arliss',\n",
              " 'arlo',\n",
              " 'arm',\n",
              " 'armada',\n",
              " 'armageddon',\n",
              " 'armed',\n",
              " 'armies',\n",
              " 'armin',\n",
              " 'armor',\n",
              " 'armored',\n",
              " 'armour',\n",
              " 'armpit',\n",
              " 'arms',\n",
              " 'army',\n",
              " 'arnie',\n",
              " 'arnold',\n",
              " 'arouse',\n",
              " 'aroused',\n",
              " 'arquette',\n",
              " 'arrange',\n",
              " 'arranged',\n",
              " 'arrangement',\n",
              " 'arrangements',\n",
              " 'arranges',\n",
              " 'arranging',\n",
              " 'array',\n",
              " 'arrest',\n",
              " 'arrested',\n",
              " 'arresting',\n",
              " 'arrests',\n",
              " 'arrival',\n",
              " 'arrivals',\n",
              " 'arrive',\n",
              " 'arrived',\n",
              " 'arrives',\n",
              " 'arriving',\n",
              " 'arrogance',\n",
              " 'arrogant',\n",
              " 'arrow',\n",
              " 'arroway',\n",
              " 'arsenal',\n",
              " 'arsinee',\n",
              " 'art',\n",
              " 'artful',\n",
              " 'artfully',\n",
              " 'arthouse',\n",
              " 'arthur',\n",
              " 'article',\n",
              " 'articles',\n",
              " 'articulate',\n",
              " 'artifacts',\n",
              " 'artifice',\n",
              " 'artificial',\n",
              " 'artificially',\n",
              " 'artisan',\n",
              " 'artist',\n",
              " 'artistic',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nviK4rm4uTSD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e37054bc-e75d-40c9-d8cc-89fa6db73140"
      },
      "source": [
        "X_count_vect = pd.DataFrame(X_count_vect.toarray(), columns=X_names)\n",
        "X_count_vect.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 17316)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUjbSlQNwTWJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "fbbe9013-d76a-4931-ef7b-9319a6638b6a"
      },
      "source": [
        "X_count_vect.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>00</th>\n",
              "      <th>000</th>\n",
              "      <th>007</th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>1000</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>105</th>\n",
              "      <th>107</th>\n",
              "      <th>11</th>\n",
              "      <th>110</th>\n",
              "      <th>12</th>\n",
              "      <th>129</th>\n",
              "      <th>13</th>\n",
              "      <th>130</th>\n",
              "      <th>137</th>\n",
              "      <th>13th</th>\n",
              "      <th>14</th>\n",
              "      <th>14th</th>\n",
              "      <th>15</th>\n",
              "      <th>150</th>\n",
              "      <th>1500s</th>\n",
              "      <th>155</th>\n",
              "      <th>16</th>\n",
              "      <th>160</th>\n",
              "      <th>161</th>\n",
              "      <th>16mm</th>\n",
              "      <th>16th</th>\n",
              "      <th>16x9</th>\n",
              "      <th>17</th>\n",
              "      <th>175</th>\n",
              "      <th>1773</th>\n",
              "      <th>17th</th>\n",
              "      <th>18</th>\n",
              "      <th>180</th>\n",
              "      <th>1800s</th>\n",
              "      <th>1839</th>\n",
              "      <th>1888</th>\n",
              "      <th>18th</th>\n",
              "      <th>...</th>\n",
              "      <th>yuppie</th>\n",
              "      <th>yuppies</th>\n",
              "      <th>yvette</th>\n",
              "      <th>zachary</th>\n",
              "      <th>zack</th>\n",
              "      <th>zahn</th>\n",
              "      <th>zane</th>\n",
              "      <th>zany</th>\n",
              "      <th>zapped</th>\n",
              "      <th>zeal</th>\n",
              "      <th>zellweger</th>\n",
              "      <th>zemeckis</th>\n",
              "      <th>zen</th>\n",
              "      <th>zero</th>\n",
              "      <th>zeroing</th>\n",
              "      <th>zest</th>\n",
              "      <th>zeta</th>\n",
              "      <th>zeus</th>\n",
              "      <th>zhang</th>\n",
              "      <th>zhou</th>\n",
              "      <th>ziggy</th>\n",
              "      <th>zingers</th>\n",
              "      <th>zip</th>\n",
              "      <th>zippel</th>\n",
              "      <th>zipper</th>\n",
              "      <th>zippy</th>\n",
              "      <th>zoe</th>\n",
              "      <th>zombie</th>\n",
              "      <th>zombies</th>\n",
              "      <th>zone</th>\n",
              "      <th>zoo</th>\n",
              "      <th>zoolander</th>\n",
              "      <th>zoom</th>\n",
              "      <th>zooming</th>\n",
              "      <th>zooms</th>\n",
              "      <th>zorg</th>\n",
              "      <th>zorro</th>\n",
              "      <th>zucker</th>\n",
              "      <th>zuko</th>\n",
              "      <th>zwick</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 17316 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   00  000  007  10  100  1000  ...  zooms  zorg  zorro  zucker  zuko  zwick\n",
              "0   1    0    0   0    0     0  ...      0     0      0       0     0      0\n",
              "1   0    0    0   0    0     0  ...      0     0      0       0     0      0\n",
              "2   0    0    0   0    0     0  ...      0     0      0       0     0      0\n",
              "3   0    1    0   0    0     0  ...      0     0      0       0     0      0\n",
              "4   0    0    0   0    0     0  ...      0     0      0       0     0      0\n",
              "\n",
              "[5 rows x 17316 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2QgEuNjFAgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# splitting to testing & training sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMK_-pJGFgEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_cv, X_test_cv, Y_train_cv, Y_test_cv = train_test_split(X_count_vect, y, test_size = 0.25, random_state = 5)\n",
        "# test size will be 25% of the whole data frame, so training size will be 75%"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8w9ZHxZF33J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27f1e000-fead-4efb-8cb5-df69fc00363d"
      },
      "source": [
        "X_train_cv.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1500, 17316)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPZkEp42GC1T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f7e548b6-24c5-427d-ddd5-eea7505f2c3e"
      },
      "source": [
        "X_test_cv.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 17316)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d6EU1xQGanO",
        "colab_type": "text"
      },
      "source": [
        "## Naive Bayes Classifier \n",
        "- classification techn based on bayes theorem on the assumption of independence among predictors \n",
        "- SO: assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jllC9xnKGGfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# so the data now has been split, will use naive bayes for text classification for both training & testing sets\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "# instantiate classifier and fit it with the training features & labels \n",
        "gnb = GaussianNB()\n",
        "y_pred_gnb = gnb.fit(X_train_cv, Y_train_cv).predict(X_test_cv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lu4y15OHNW-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e3c2895e-cd3c-4fa9-a410-6b12933435c6"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf_cv = MultinomialNB()\n",
        "clf_cv.fit(X_train_cv, Y_train_cv)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPVxP6aEHdgS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0fdd204e-e624-4bb2-8178-43d41a5f1b8a"
      },
      "source": [
        "# use predict function to pass the training feature \n",
        "y_pred_cv = clf_cv.predict(X_test_cv)\n",
        "type(y_pred_cv)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSZCEnX3HvzN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cb84ee04-e81e-4fd9-db47-e64be64f95a7"
      },
      "source": [
        "print(metrics.accuracy_score(Y_test_cv, y_pred_cv))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36jgciBpH3tK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3ac6817e-3928-4a4c-b89e-599d50b215d3"
      },
      "source": [
        "score_clf_cv = confusion_matrix(Y_test_cv, y_pred_cv)\n",
        "score_clf_cv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[258,   0],\n",
              "       [  0, 242]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    }
  ]
}